# LLM Station dla UM790 Pro

Offline Å›rodowisko LLM z obsÅ‚ugÄ… GPU AMD Radeon 780M (RDNA3) uruchamiane z USB.

## ğŸ“‹ Spis treÅ›ci

- [Wymagania sprzÄ™towe](#wymagania-sprzÄ™towe)
- [Architektura](#architektura)
- [Szybki start](#szybki-start)
- [Åšrodowisko 1: Ollama + Open-WebUI](#Å›rodowisko-1-ollama--open-webui)
- [Åšrodowisko 2: llama.cpp + ROCm](#Å›rodowisko-2-llamacpp--rocm)
- [Tworzenie bootowalnego USB](#tworzenie-bootowalnego-usb)
- [PorÃ³wnanie wydajnoÅ›ci](#porÃ³wnanie-wydajnoÅ›ci)

## Wymagania sprzÄ™towe

| Komponent | Specyfikacja |
|-----------|--------------|
| **CPU** | AMD Ryzen 9 7940HS (UM790 Pro) |
| **GPU** | AMD Radeon 780M (RDNA3, 12 CU) |
| **RAM** | 16GB DDR5 |
| **USB** | 64GB USB 3.2 |
| **WyÅ›wietlacz** | HDMI/USB4, max 4K |

## Architektura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              USB 64GB (bootowalne)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Linux (Fedora/Ubuntu) â†’ Å‚adowany do RAM        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”œâ”€ Podman                                      â”‚
â”‚  â”œâ”€ ROCm drivers (GPU offload)                  â”‚
â”‚  â”œâ”€ Ollama (port 11434) lub llama.cpp (8080)    â”‚
â”‚  â”œâ”€ Open-WebUI (port 3000)                      â”‚
â”‚  â””â”€ Pre-downloaded models (~20-40GB)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  HDMI/USB4 â†’ WyÅ›wietlacz 4K                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Szybki start

### Na istniejÄ…cym systemie (bez USB)

```bash
# 1. Sklonuj/przejdÅº do katalogu
cd environments/ollama-webui

# 2. Skonfiguruj hosta (jednorazowo)
sudo ./setup-host.sh

# 3. Pobierz modele (wymaga internetu)
./download-models.sh

# 4. Uruchom
./start.sh

# 5. OtwÃ³rz przeglÄ…darkÄ™
xdg-open http://localhost:3000
```

### Tworzenie bootowalnego USB (offline)

```bash
# 1. Przygotuj wszystkie zasoby (wymaga internetu)
cd environments/usb-builder
./prepare-offline.sh

# 2. UtwÃ³rz bootowalne USB
sudo ./build-usb.sh /dev/sdX

# 3. Boot z USB na UM790 Pro
# 4. System uruchomi siÄ™ automatycznie
```

---

## Åšrodowisko 1: Ollama + Open-WebUI

**Zalecane dla:** ÅatwoÅ›ci uÅ¼ycia, stabilnoÅ›ci, wielu modeli jednoczeÅ›nie.

### Struktura

```
ollama-webui/
â”œâ”€â”€ docker-compose.yml    # Konfiguracja kontenerÃ³w
â”œâ”€â”€ setup-host.sh         # Instalacja ROCm na hoÅ›cie
â”œâ”€â”€ download-models.sh    # Pobieranie modeli
â”œâ”€â”€ start.sh              # Uruchomienie
â”œâ”€â”€ stop.sh               # Zatrzymanie
â”œâ”€â”€ models/               # Modele Ollama (auto)
â””â”€â”€ webui-data/           # Dane Open-WebUI
```

### Porty

| UsÅ‚uga | Port | URL |
|--------|------|-----|
| Ollama API | 11434 | http://localhost:11434 |
| Open-WebUI | 3000 | http://localhost:3000 |

### Polecenia

```bash
# Start
./start.sh

# Stop
./stop.sh

# SprawdÅº status
podman ps

# Logi
podman logs ollama
podman logs open-webui

# Dodaj model (online)
podman exec ollama ollama pull llama3.2:3b

# Lista modeli
podman exec ollama ollama list
```

### Zalecane modele dla 16GB RAM

| Model | Rozmiar | Przypadek uÅ¼ycia |
|-------|---------|------------------|
| llama3.2:3b | ~2GB | Szybkie odpowiedzi |
| phi3:mini | ~2GB | Microsoft, szybki |
| mistral:7b | ~4GB | Zbalansowany |
| codellama:7b | ~4GB | Programowanie |

---

## Åšrodowisko 2: llama.cpp + ROCm

**Zalecane dla:** Maksymalnej wydajnoÅ›ci, peÅ‚nej kontroli, benchmarkÃ³w.

### Struktura

```
llama-cpp-rocm/
â”œâ”€â”€ Dockerfile            # Obraz z ROCm + llama.cpp
â”œâ”€â”€ docker-compose.yml    # Konfiguracja
â”œâ”€â”€ download-models.sh    # Pobieranie GGUF
â”œâ”€â”€ start.sh              # Uruchomienie
â”œâ”€â”€ stop.sh               # Zatrzymanie
â”œâ”€â”€ benchmark.sh          # Test wydajnoÅ›ci
â””â”€â”€ models/               # Modele GGUF
    â””â”€â”€ model.gguf        # Symlink do domyÅ›lnego
```

### Porty

| UsÅ‚uga | Port | URL |
|--------|------|-----|
| llama-server API | 8080 | http://localhost:8080 |
| OpenAI-compatible | 8080 | http://localhost:8080/v1 |

### Polecenia

```bash
# Start (domyÅ›lny model)
./start.sh

# Start z konkretnym modelem
./start.sh mistral-7b-q4.gguf

# Benchmark
./benchmark.sh

# Test API
curl http://localhost:8080/v1/models

# Chat
curl http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"messages":[{"role":"user","content":"Hello!"}]}'
```

### Zalecane modele GGUF (Q4_K_M)

| Model | Rozmiar | Tokeny/s (780M) |
|-------|---------|-----------------|
| Llama 3.2 3B | ~2GB | 15-25 t/s |
| Phi-3 Mini | ~2GB | 15-25 t/s |
| Mistral 7B | ~4GB | 8-12 t/s |
| CodeLlama 7B | ~4GB | 8-12 t/s |

---

## Tworzenie bootowalnego USB

### Przygotowanie (wymaga internetu)

```bash
cd environments/usb-builder

# 1. Pobierz wszystko na offline
./prepare-offline.sh

# To pobierze:
# - Obrazy kontenerÃ³w (~8GB)
# - Modele Ollama (~10-20GB)
# - Modele GGUF (~10-20GB)
```

### Tworzenie USB

```bash
# ZnajdÅº USB (np. /dev/sdb)
lsblk

# UtwÃ³rz USB (UWAGA: kasuje dane!)
sudo ./build-usb.sh /dev/sdX
```

### Tworzenie ISO (dla Balena Etcher)

```bash
# UtwÃ³rz bootowalne ISO
sudo ./build-iso.sh

# ISO zostanie zapisane w:
# environments/usb-builder/output/llm-station-um790pro.iso

# UÅ¼yj z Balena Etcher:
# 1. OtwÃ³rz Balena Etcher
# 2. Wybierz plik ISO
# 3. Wybierz dysk USB
# 4. Flash!
```

### Bootowanie na UM790 Pro

1. WÅ‚Ã³Å¼ USB do UM790 Pro
2. WejdÅº do BIOS (F2/Del podczas startu)
3. Ustaw boot z USB
4. System zaÅ‚aduje siÄ™ do RAM
5. PrzeglÄ…darka otworzy siÄ™ automatycznie

### Pierwszy boot (jednorazowo)

```bash
# Na USB-bootowanym systemie
sudo /run/media/*/LLM_DATA/setup-first-boot.sh
sudo /run/media/*/LLM_DATA/usb-builder/install-autostart.sh
```

---

## PorÃ³wnanie wydajnoÅ›ci

### vLLM vs Ollama vs llama.cpp na 780M

| Aspekt | vLLM | Ollama | llama.cpp |
|--------|------|--------|-----------|
| ROCm 780M | âš ï¸ Problematyczne | âœ… Natywne | âœ… Stabilne |
| ÅatwoÅ›Ä‡ uÅ¼ycia | âŒ Trudne | âœ… Åatwe | âš¡ Åšrednie |
| WydajnoÅ›Ä‡ 7B | ~5 t/s | ~8-10 t/s | ~8-12 t/s |
| Multi-model | âœ… | âœ… | âŒ |
| PamiÄ™Ä‡ | Wysoka | Åšrednia | Niska |

### Rekomendacja

- **Codzienne uÅ¼ycie:** Ollama + Open-WebUI
- **Maksymalna wydajnoÅ›Ä‡:** llama.cpp
- **Unikaj:** vLLM na RDNA3 (niestabilne)

---

## RozwiÄ…zywanie problemÃ³w

### GPU nie wykryte

```bash
# SprawdÅº urzÄ…dzenia
ls -la /dev/kfd /dev/dri

# SprawdÅº ROCm
rocm-smi --showproductname

# Dodaj uÅ¼ytkownika do grupy video
sudo usermod -aG video $USER
# Wyloguj i zaloguj ponownie
```

### BÅ‚Ä™dy ROCm na 780M

```bash
# Ustaw wersjÄ™ GFX (RDNA3 = 11.0.0)
export HSA_OVERRIDE_GFX_VERSION=11.0.0
```

### Kontener nie startuje

```bash
# SprawdÅº logi
podman logs ollama
podman logs llama-server

# Restart
./stop.sh && ./start.sh
```

### Brak pamiÄ™ci

```bash
# UÅ¼yj mniejszego modelu
# Ollama:
podman exec ollama ollama pull phi3:mini

# llama.cpp:
./start.sh phi-3-mini-q4.gguf
```

---

## Licencja

MIT License - zobacz gÅ‚Ã³wny plik LICENSE projektu.
