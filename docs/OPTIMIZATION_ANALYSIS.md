# Live Narrator - Analiza Optymalizacji

## Obecna Architektura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     LIVE NARRATOR PIPELINE                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ  RTSP Stream                                                    ‚îÇ
‚îÇ      ‚îÇ                                                          ‚îÇ
‚îÇ      ‚ñº                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ  ‚îÇ FastCapture ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Smart       ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Frame       ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ (OpenCV/    ‚îÇ    ‚îÇ Detector    ‚îÇ    ‚îÇ Optimizer   ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  FFmpeg)    ‚îÇ    ‚îÇ (YOLO+HOG)  ‚îÇ    ‚îÇ (resize,    ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  ~0-5ms     ‚îÇ    ‚îÇ  ~40-700ms  ‚îÇ    ‚îÇ  compress)  ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ  ~50ms      ‚îÇ         ‚îÇ
‚îÇ                            ‚îÇ           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ                            ‚îÇ                  ‚îÇ                 ‚îÇ
‚îÇ                            ‚ñº                  ‚ñº                 ‚îÇ
‚îÇ                     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ                     ‚îÇ Skip if     ‚îÇ    ‚îÇ Vision LLM  ‚îÇ         ‚îÇ
‚îÇ                     ‚îÇ no motion   ‚îÇ    ‚îÇ (llava:7b)  ‚îÇ         ‚îÇ
‚îÇ                     ‚îÇ or no target‚îÇ    ‚îÇ ~1.5-3s     ‚îÇ         ‚îÇ
‚îÇ                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ                                               ‚îÇ                 ‚îÇ
‚îÇ                                               ‚ñº                 ‚îÇ
‚îÇ                                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ                                        ‚îÇ Guarder LLM ‚îÇ         ‚îÇ
‚îÇ                                        ‚îÇ (gemma:2b)  ‚îÇ         ‚îÇ
‚îÇ                                        ‚îÇ ~200-500ms  ‚îÇ         ‚îÇ
‚îÇ                                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ                                               ‚îÇ                 ‚îÇ
‚îÇ                                               ‚ñº                 ‚îÇ
‚îÇ                                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ                                        ‚îÇ TTS Output  ‚îÇ         ‚îÇ
‚îÇ                                        ‚îÇ (pyttsx3)   ‚îÇ         ‚îÇ
‚îÇ                                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Obecne Czasy (benchmark)

| Component | Czas | Status |
|-----------|------|--------|
| FastCapture | 0-5ms | ‚úÖ Optimal |
| YOLO Detection | 10-50ms (GPU) | ‚úÖ Optimal |
| Smart Detect (HOG fallback) | 700ms | ‚ö†Ô∏è Wolne |
| Frame Optimize | 50ms | ‚úÖ OK |
| Vision LLM (llava:7b) | 1.5-3s | ‚ö†Ô∏è Bottleneck |
| Guarder LLM (gemma:2b) | 200-500ms | ‚úÖ OK |
| **Total Cycle** | **2-4s** | - |

## Propozycje Optymalizacji

### 1. üöÄ Batch Processing (Grupowanie klatek)

**Problem**: Ka≈ºda klatka analizowana osobno przez LLM.

**RozwiƒÖzanie**: Grupuj 3-5 klatek i analizuj razem.

```python
class BatchFrameAnalyzer:
    def __init__(self, batch_size=3):
        self.batch_size = batch_size
        self.frame_buffer = []
    
    def add_frame(self, frame_path):
        self.frame_buffer.append(frame_path)
        if len(self.frame_buffer) >= self.batch_size:
            return self._analyze_batch()
        return None
    
    def _analyze_batch(self):
        # Stw√≥rz grid 3 klatek w jednym obrazie
        # LLM analizuje wszystkie naraz
        grid = self._create_grid(self.frame_buffer)
        prompt = "Analyze these 3 consecutive frames. Describe any movement or changes."
        result = llm.analyze(grid, prompt)
        self.frame_buffer.clear()
        return result
```

**Korzy≈õci**:
- 3x mniej wywo≈Ça≈Ñ LLM
- Lepsze wykrywanie ruchu (kontekst)
- ~1s na 3 klatki zamiast ~3s

---

### 2. üéØ Hierarchiczne Przetwarzanie

**Problem**: Ka≈ºda klatka przechodzi pe≈Çny pipeline.

**RozwiƒÖzanie**: 3-poziomowa hierarchia:

```
Level 1: YOLO Only (10ms)
    ‚îú‚îÄ‚îÄ No detection ‚Üí Skip
    ‚îî‚îÄ‚îÄ Detection ‚Üí Level 2

Level 2: Fast LLM (moondream, 300ms)
    ‚îú‚îÄ‚îÄ Low confidence ‚Üí Level 3
    ‚îî‚îÄ‚îÄ High confidence ‚Üí Output

Level 3: Accurate LLM (llava:7b, 1.5s)
    ‚îî‚îÄ‚îÄ Final analysis
```

```python
class HierarchicalAnalyzer:
    def analyze(self, frame):
        # Level 1: YOLO
        detections = self.yolo.detect(frame)
        if not detections:
            return None
        
        # Level 2: Fast check
        fast_result = self.fast_llm.analyze(frame)
        if self._is_confident(fast_result):
            return fast_result
        
        # Level 3: Accurate analysis
        return self.accurate_llm.analyze(frame)
```

---

### 3. üìä Keyframe Extraction (Ekstrakcja kluczowych klatek)

**Problem**: Analizujemy co N sekund, nawet je≈õli nic siƒô nie zmieni≈Ço.

**RozwiƒÖzanie**: Wykrywaj "keyframes" na podstawie zmian:

```python
class KeyframeExtractor:
    def __init__(self, threshold=0.15):
        self.threshold = threshold
        self.last_keyframe = None
        self.last_histogram = None
    
    def is_keyframe(self, frame):
        histogram = cv2.calcHist([frame], [0,1,2], None, [8,8,8], [0,256,0,256,0,256])
        
        if self.last_histogram is None:
            self.last_histogram = histogram
            return True
        
        # Compare histograms
        diff = cv2.compareHist(self.last_histogram, histogram, cv2.HISTCMP_BHATTACHARYYA)
        
        if diff > self.threshold:
            self.last_histogram = histogram
            return True
        
        return False
```

---

### 4. üîÑ Async Pipeline (R√≥wnoleg≈Çe przetwarzanie)

**Problem**: Sekwencyjne przetwarzanie blokuje capture.

**RozwiƒÖzanie**: Oddzielne wƒÖtki dla ka≈ºdego etapu:

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

class AsyncPipeline:
    def __init__(self):
        self.capture_queue = asyncio.Queue(maxsize=5)
        self.analysis_queue = asyncio.Queue(maxsize=3)
        self.executor = ThreadPoolExecutor(max_workers=4)
    
    async def capture_loop(self):
        while True:
            frame = await self._capture_frame()
            await self.capture_queue.put(frame)
    
    async def detection_loop(self):
        while True:
            frame = await self.capture_queue.get()
            if self._has_motion(frame):
                await self.analysis_queue.put(frame)
    
    async def analysis_loop(self):
        while True:
            frame = await self.analysis_queue.get()
            # Run LLM in thread pool (non-blocking)
            result = await asyncio.get_event_loop().run_in_executor(
                self.executor, self._analyze_with_llm, frame
            )
            self._output(result)
```

---

### 5. üóúÔ∏è Smart Compression (Inteligentna kompresja)

**Problem**: Wysy≈Çamy pe≈Çne obrazy do LLM.

**RozwiƒÖzanie**: Kompresuj tylko interesujƒÖce regiony:

```python
class SmartCompressor:
    def compress_for_llm(self, frame, detections):
        if not detections:
            # No detections - send thumbnail
            return cv2.resize(frame, (256, 256))
        
        # Crop to detection region with padding
        x, y, w, h = self._get_bounding_box(detections)
        padding = 50
        crop = frame[
            max(0, y-padding):min(frame.shape[0], y+h+padding),
            max(0, x-padding):min(frame.shape[1], x+w+padding)
        ]
        
        # Resize to optimal size for LLM
        return cv2.resize(crop, (384, 384))
```

---

### 6. üìù Response Caching (Cache odpowiedzi)

**Problem**: Te same sceny analizowane wielokrotnie.

**RozwiƒÖzanie**: Cache na podstawie visual hash:

```python
import imagehash
from PIL import Image

class ResponseCache:
    def __init__(self, ttl=30):
        self.cache = {}
        self.ttl = ttl
    
    def get_or_analyze(self, frame_path, analyzer):
        # Compute perceptual hash
        img = Image.open(frame_path)
        phash = str(imagehash.phash(img))
        
        # Check cache
        if phash in self.cache:
            entry = self.cache[phash]
            if time.time() - entry['time'] < self.ttl:
                return entry['response']
        
        # Analyze and cache
        response = analyzer(frame_path)
        self.cache[phash] = {'response': response, 'time': time.time()}
        return response
```

---

### 7. üé¨ Scene Segmentation (Segmentacja scen)

**Problem**: Nie rozr√≥≈ºniamy typ√≥w scen.

**RozwiƒÖzanie**: Klasyfikuj scenƒô i u≈ºyj odpowiedniego pipeline:

```python
class SceneClassifier:
    SCENES = {
        'static': {'interval': 10, 'llm': 'moondream'},
        'low_activity': {'interval': 5, 'llm': 'moondream'},
        'high_activity': {'interval': 2, 'llm': 'llava:7b'},
        'emergency': {'interval': 0.5, 'llm': 'llava:13b'},
    }
    
    def classify(self, frame_analysis):
        motion = frame_analysis.get('motion_percent', 0)
        person_count = len(frame_analysis.get('detections', []))
        
        if motion > 30 or person_count > 2:
            return 'high_activity'
        elif motion > 5 or person_count > 0:
            return 'low_activity'
        else:
            return 'static'
    
    def get_config(self, scene_type):
        return self.SCENES.get(scene_type, self.SCENES['static'])
```

---

### 8. üîä Streaming LLM Response

**Problem**: Czekamy na pe≈ÇnƒÖ odpowied≈∫ LLM.

**RozwiƒÖzanie**: Stream tokens i m√≥w/wy≈õwietlaj na bie≈ºƒÖco:

```python
class StreamingNarrator:
    async def narrate_streaming(self, frame_path):
        prompt = self._build_prompt(frame_path)
        
        buffer = ""
        async for token in self.llm.stream(prompt, image=frame_path):
            buffer += token
            
            # Speak complete sentences
            if '.' in buffer or '!' in buffer:
                sentence, buffer = buffer.rsplit('.', 1)
                await self.tts.speak_async(sentence + '.')
```

---

## Implementacja Priorytetowa

### Faza 1 (Quick Wins) - 1-2 dni
1. ‚úÖ Zmiana modelu na llava:7b
2. [ ] Keyframe extraction
3. [ ] Smart compression (crop to detection)

### Faza 2 (Medium Impact) - 3-5 dni
4. [ ] Hierarchical processing
5. [ ] Response caching
6. [ ] Scene classification

### Faza 3 (Advanced) - 1-2 tygodnie
7. [ ] Batch processing
8. [ ] Async pipeline
9. [ ] Streaming LLM response

---

## Benchmark Targets

| Optymalizacja | Obecny czas | Target | Poprawa |
|---------------|-------------|--------|---------|
| Keyframes | 2-4s/frame | 2-4s/keyframe | 50% mniej LLM calls |
| Hierarchical | 2-4s always | 300ms-2s | Adaptacyjny |
| Batch (3 frames) | 6-12s | 3-4s | 60% szybciej |
| Caching | 2-4s repeated | 0ms cached | 90%+ dla static |
| **Combined** | **2-4s** | **0.5-2s avg** | **~3x faster** |

---

## Quick Start - W≈ÇƒÖczenie optymalizacji

```bash
# U≈ºyj llava:7b (domy≈õlnie teraz)
sq live narrator --url "rtsp://camera/stream" --mode track --focus person --tts

# Z adaptive intervals
sq live narrator --url "rtsp://camera/stream" --mode track --focus person --tts --adaptive

# Fast mode (moondream + aggressive caching)
sq live narrator --url "rtsp://camera/stream" --mode track --focus person --tts --fast

# High accuracy (llava:13b)
sq live narrator --url "rtsp://camera/stream" --mode track --focus person --tts --model llava:13b
```
